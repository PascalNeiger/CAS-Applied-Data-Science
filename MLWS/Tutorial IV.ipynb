{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial IV: Convolutions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>\n",
    "Bern Winter School on Machine Learning, 28.01-01.02 2019<br>\n",
    "Mykhailo Vladymyrov\n",
    "</p>\n",
    "\n",
    "This work is licensed under a <a href=\"http://creativecommons.org/licenses/by-nc-sa/4.0/\">Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License</a>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this session we will look at the convolutoin operation and try to build some intuition about it.\n",
    "Also we will look at one of the state-of-the art deep models, [Inception](https://arxiv.org/abs/1602.07261). It is designed to perform image recognition."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## download libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! wget http://scits-training.unibe.ch/data/tut_files/t4.tgz\n",
    "! tar -xvzf t4.tgz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import IPython.display as ipyd\n",
    "import tensorflow as tf\n",
    "from PIL import Image\n",
    "\n",
    "# We'll tell matplotlib to inline any drawn figures like so:\n",
    "%matplotlib inline\n",
    "plt.style.use('ggplot')\n",
    "from utils import gr_disp\n",
    "from utils import inception\n",
    "\n",
    "from IPython.core.display import HTML\n",
    "HTML(\"\"\"<style> .rendered_html code { \n",
    "    padding: 2px 5px;\n",
    "    color: #0000aa;\n",
    "    background-color: #cccccc;\n",
    "} </style>\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Convolutions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In fully connected network all inputs are cnnected to all neurons of next layer:\n",
    "<tr>\n",
    "    <td> <img src=\"http://scits-training.unibe.ch/data/figures/Perceptron.png\" alt=\"drawing\" width=\"100px\"/></td><br>\n",
    "    <td> <img src=\"http://scits-training.unibe.ch/data/figures/MLP.png\" alt=\"drawing\" width=\"300px\"/></td> \n",
    "</tr> \n",
    "<br>In convolutional nets the same holds for each neighbourhood, and the weights are shared:<br>\n",
    "![CNN1.png](http://scits-training.unibe.ch/data/figures/CNN1.png)<br>\n",
    "![CNN2.png](http://scits-training.unibe.ch/data/figures/CNN2.png)<br>\n",
    "![CNN3.png](http://scits-training.unibe.ch/data/figures/CNN3.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see what a convolution is, and how it behaves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load image, convert to gray-scale and normalize\n",
    "img_raw = plt.imread('ML3/chelsea.jpg').mean(axis=2)[-256:, 100:356].astype(np.float32)\n",
    "img_raw = (img_raw-img_raw.mean())/img_raw.std()\n",
    "\n",
    "plt.imshow(img_raw, cmap='gray')\n",
    "plt.grid(False)\n",
    "img_raw4d = img_raw[np.newaxis,...,np.newaxis]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = tf.Graph()\n",
    "with g.as_default():\n",
    "    #convolve x 5 times with a 5x5 filter\n",
    "    x = tf.placeholder(dtype=tf.float32, shape=(1,256,256,1),name='img')\n",
    "    flt = tf.placeholder(dtype=tf.float32, shape=(5,5,1,1), name='flt')\n",
    "    y1 = tf.nn.conv2d(x , flt, strides=[1,1,1,1], padding='VALID', name='convolved') #[1,2,2,1]\n",
    "    y2 = tf.nn.conv2d(y1, flt, strides=[1,1,1,1], padding='VALID', name='convolved')\n",
    "    y3 = tf.nn.conv2d(y2, flt, strides=[1,1,1,1], padding='VALID', name='convolved')\n",
    "    y4 = tf.nn.conv2d(y3, flt, strides=[1,1,1,1], padding='VALID', name='convolved')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flt_mtx = [\n",
    "    [ 0, 0, 0, 0, 0,],\n",
    "    [ 0, 0, 0, 0, 0,],\n",
    "    [ 0, 0, 1, 0, 0,],\n",
    "    [ 0, 0, 0, 0, 0,],\n",
    "    [ 0, 0, 0, 0, 0,],\n",
    "]\n",
    "with tf.Session(graph=g) as sess:\n",
    "    flt_mtx_np = np.array(flt_mtx, np.float32)\n",
    "    flt_mtx_np = flt_mtx_np[..., np.newaxis, np.newaxis]\n",
    "    res = sess.run([x,y1,y2,y3,y4], feed_dict={x:img_raw4d, flt:flt_mtx_np})\n",
    "res = [r[0,...,0] for r in res]\n",
    "n = len(res)\n",
    "fig, ax = plt.subplots(1, n, figsize=(n*4, 4))\n",
    "for col in range(n):\n",
    "    ax[col].imshow(res[col], cmap='gray')\n",
    "    ax[col].grid(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def conv_2D(x, n_output_ch,\n",
    "            k_w=3, k_h=3,\n",
    "            s_x=1, s_y=1,\n",
    "            activation=tf.nn.relu,\n",
    "            padding='VALID', name='conv2d', reuse=None\n",
    "           ):\n",
    "    \"\"\"\n",
    "    Helper for creating a 2d convolution operation.\n",
    "\n",
    "    Args:\n",
    "        x (tf.Tensor): Input tensor to convolve.\n",
    "        n_output_ch (int): Number of filters.\n",
    "        k_w (int): Kernel width\n",
    "        k_h (int): Kernel height\n",
    "        s_x (int): Width stride\n",
    "        s_y (int): Height stride\n",
    "        activation (tf.Function): activation function to apply to the convolved data\n",
    "        padding (str): Padding type: 'SAME' or 'VALID'\n",
    "        name (str): Variable scope\n",
    "        reuse (tf.Flag): Flag whether to use existing variable. Can be False(None), True, or tf.AUTO_REUSE\n",
    "\n",
    "    Returns:\n",
    "        op (tf.Tensor, tf.Tensor, tf.Tensor, tf.Tensor): Output of activation, convolution, weights, bias\n",
    "    \"\"\"\n",
    "    with tf.variable_scope(name or 'conv2d', reuse=reuse):\n",
    "        w = tf.get_variable(name='W',\n",
    "                            shape=[k_h, k_w, x.get_shape()[-1], n_output_ch],\n",
    "                            initializer=tf.contrib.layers.xavier_initializer()\n",
    "                           )\n",
    "        \n",
    "        wx = tf.nn.conv2d(name='conv',\n",
    "                          input=x, filter=w,\n",
    "                          strides=[1, s_y, s_x, 1],\n",
    "                          padding=padding\n",
    "                         )\n",
    "        \n",
    "        b = tf.get_variable(name='b',\n",
    "                            shape=[n_output_ch], initializer=tf.constant_initializer(value=0.0)\n",
    "                           )\n",
    "        h = tf.nn.bias_add(name='h',\n",
    "                           value=wx,\n",
    "                           bias=b\n",
    "                          )\n",
    "\n",
    "        if activation is not None:\n",
    "            x = activation(h, name=activation.__name__)\n",
    "        else:\n",
    "            x = h\n",
    "    \n",
    "    return x, h, w, b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Load the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "inception module here is a small module that performs loading the inception model as well as image preparation for the training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net, net_labels = inception.get_inception_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get model graph definition and change it to use GPU\n",
    "gd = net\n",
    "\n",
    "str_dg = gd.SerializeToString()\n",
    "#uncomment next line to use GPU acceleration\n",
    "#str_dg = str_dg.replace(b'/cpu:0', b'/gpu:0') #a bit extreme approach, but works =)\n",
    "gd = gd.FromString(str_dg)\n",
    "\n",
    "gr_disp.show(gd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Create the graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This whole model won't fit in GPU memory. We will take only the part from input to the main output and copy it to a second graph, that we will use further."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gd2 = tf.graph_util.extract_sub_graph(gd, ['output'])\n",
    "g2 = tf.Graph() # full graph\n",
    "with g2.as_default():\n",
    "    tf.import_graph_def(gd2, name='inception')\n",
    "\n",
    "gr_disp.show(g2.as_graph_def())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Test the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use one image to check model. `img_preproc` is croped to 256x256 pixels and slightly transformed to be used as imput for the model using `inception.prepare_training_img`. `inception.training_img_to_display` is then used to convert it to displayable one.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_raw = plt.imread('ML3/chelsea.jpg')\n",
    "img_preproc = inception.prepare_training_img(img_raw)\n",
    "img_deproc = inception.training_img_to_display(img_preproc)\n",
    "_, axs = plt.subplots(1, 2, figsize=(10,5))\n",
    "axs[0].imshow(img_raw)\n",
    "axs[0].grid(False)\n",
    "axs[1].imshow(img_deproc)\n",
    "axs[1].grid(False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then get the input and output tensors, and obtain probabilities of each class on this image:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from graph we will get the input and output tensors. \n",
    "#Any tensor and operation can be obtained by name\n",
    "g2.device('/gpu:0')\n",
    "with g2.as_default():\n",
    "    x = g2.get_tensor_by_name('inception/input:0')\n",
    "    softmax = g2.get_tensor_by_name('inception/output:0')\n",
    "    \n",
    "#Then we will feed the image in the graph and print 5 classes that have highest probability\n",
    "with tf.Session(graph=g2) as sess:\n",
    "    res = np.squeeze(sess.run(softmax, feed_dict={x: img_preproc[np.newaxis]}))\n",
    "    \n",
    "    indexes_sorted_by_probability = res.argsort()[::-1]\n",
    "    print([(res[idx], net_labels[idx])\n",
    "           for idx in indexes_sorted_by_probability[:5]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
